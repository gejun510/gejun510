# 十分类神经网络的损失函数计算

## 1. 输出层与 Softmax

神经网络最后一层输出一个长度为 10 的向量（称为 *logits*）：

$$
z = [z_1, z_2, \ldots, z_{10}]
$$

每个 $ z_i $ 表示类别 $ i $ 的未归一化得分。  
通过 **Softmax** 函数，将其转化为概率分布：

$$
p_i = \frac{e^{z_i}}{\sum_{j=1}^{10} e^{z_j}}
$$

满足  0 < p_i < 1  且 $\sum_i p_i = 1$。

---

## 2. 标签表示（One-hot 编码）

真实标签用 one-hot 向量表示：

$$
y = [y_1, y_2, \ldots, y_{10}]
$$

若真实类别为第 $k$ 类，则：

$$
y_k = 1, \quad y_{i \neq k} = 0
$$

---

## 3. 交叉熵损失定义

对单个样本，交叉熵损失定义为：

$$
L = -\sum_{i=1}^{10} y_i \log(p_i)
$$

由于只有正确类别 $k$ 的 $y_k=1$，其余为 0，因此简化为：

$$
L = -\log(p_k)
$$

---

## 4. 批量样本的平均损失

若批量大小为 $N$，则总损失为：

$$
L = -\frac{1}{N} \sum_{n=1}^{N} \sum_{i=1}^{10} y_i^{(n)} \log(p_i^{(n)})
$$

---

## 5. 示例计算

假设网络输出 logits：

$$
z = [2.3, 0.1, -1.5, 0.3, 1.2, 0.7, -0.8, 0.4, 1.1, -0.2]
$$

真实类别为第 4 类（即 $y_4 = 1$）。  
则先计算 Softmax 概率：

$$
p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

然后取第 4 类的概率：

$$
L = -\log(p_4)
$$

---

## 6. PyTorch 实现

```python
import torch
import torch.nn.functional as F

# logits: [batch_size, 10]
# targets: [batch_size], 每个元素是类别索引(0~9)
loss = F.cross_entropy(logits, targets)

